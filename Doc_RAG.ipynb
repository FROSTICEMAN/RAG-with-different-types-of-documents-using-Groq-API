{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided context, it appears to be a table of financial data. \\n\\nIt includes information like:\\n\\n* **Fund Names:**  \"1 Year T Bill (Additional Benchmark)\" and \"ICICI Prudential Corporate Bond Fund\"\\n* **Numerical Values:**  These likely represent financial metrics like yields or returns.\\n* **Dates:**  \"2011-04-05 00:00:00\" is associated with the second fund.\\n\\n\\nWithout more context, it\\'s difficult to say for sure what the specific document is about. \\n' response_metadata={'token_usage': {'completion_tokens': 123, 'prompt_tokens': 183, 'total_tokens': 306, 'completion_time': 0.223636364, 'prompt_time': 0.009945688, 'queue_time': 0.005479221000000001, 'total_time': 0.233582052}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-52ccc3d9-8a7c-42bd-b181-33282fbf4abe-0' usage_metadata={'input_tokens': 183, 'output_tokens': 123, 'total_tokens': 306}\n"
     ]
    }
   ],
   "source": [
    "## Merging two Documents i.e Pdf and xlsx\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Extract data from Excel\n",
    "def extract_data_from_excel(excel_path):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    return df.to_string()  # Convert DataFrame to string directly\n",
    "\n",
    "# Combine PDF text and Excel data\n",
    "def combine_data(pdf_text, excel_text):\n",
    "    return f\"{pdf_text}\\n\\n{excel_text}\"\n",
    "\n",
    "# Split text into chunks\n",
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Extract and process data\n",
    "pdf_text = extract_text_from_pdf(r\"C:\\Users\\Pritish\\Downloads\\Complete (2).pdf\")\n",
    "excel_text = extract_data_from_excel(r\"C:\\Users\\Pritish\\Downloads\\Yearly_Returns.xlsx\")\n",
    "\n",
    "merged_data = combine_data(pdf_text, excel_text)\n",
    "\n",
    "# Split the merged data into chunks\n",
    "doc_splits = split_text_into_chunks(merged_data)\n",
    "\n",
    "# Convert chunks into Document objects\n",
    "documents = [Document(page_content=chunk) for chunk in doc_splits]\n",
    "\n",
    "# Create and configure the vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Retrieve from the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Groq API\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Configure the Groq LLM\n",
    "groq_api_key = \"xxx\"\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Gemma2-9b-It\", temperature=0)\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"I will provide you pieces of [Context] to answer the [Question]. \n",
    "If you don't know the answer based on [Context], just say that you don't know. Don't make up an answer.\n",
    "[Context]: {context}\n",
    "[Question]: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Example question\n",
    "question = \"What is the document about\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "context_from_docs = retrieved_docs[0].page_content  # Assuming you want the first relevant document's content\n",
    "\n",
    "# Create the final prompt using the template\n",
    "final_prompt = prompt.format(context=context_from_docs, question=question)\n",
    "\n",
    "# Invoke the LLM directly with the formatted prompt\n",
    "response = llm.invoke(final_prompt)\n",
    "\n",
    "# Print the result\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided context, the document appears to be about financial instruments and their performance. \\n\\nIt lists various items, including:\\n\\n* **1 Year T Bill (Additional Benchmark):**  Likely a benchmark for short-term interest rates.\\n* **ICICI Prudential Floating Interest Fund:** A type of mutual fund that invests in floating-rate debt securities.\\n* **NIFTY Low Duration Debt Index A-I (Benchmark):** A benchmark index tracking low-duration debt securities.\\n\\nEach entry includes numerical values that could represent yields, returns, or other financial metrics.  \\n\\n\\nLet me know if you have more context or specific questions! \\n' response_metadata={'token_usage': {'completion_tokens': 136, 'prompt_tokens': 281, 'total_tokens': 417, 'completion_time': 0.247272727, 'prompt_time': 0.045158853, 'queue_time': 0.429282371, 'total_time': 0.29243158}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-62c51c40-eb43-46c5-9926-50ba8b2cac62-0' usage_metadata={'input_tokens': 281, 'output_tokens': 136, 'total_tokens': 417}\n"
     ]
    }
   ],
   "source": [
    "## Routing two Documents i.e Pdf and xlsx\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Extract data from Excel\n",
    "def extract_data_from_excel(excel_path):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    return df.to_string()  # Convert DataFrame to string directly\n",
    "\n",
    "# Split text into chunks\n",
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Process PDF data\n",
    "pdf_text = extract_text_from_pdf(r\"C:\\Users\\Pritish\\Downloads\\Complete (2).pdf\")\n",
    "pdf_chunks = split_text_into_chunks(pdf_text)\n",
    "pdf_documents = [Document(page_content=chunk) for chunk in pdf_chunks]\n",
    "\n",
    "# Create vector store for PDF data\n",
    "pdf_vectorstore = Chroma.from_documents(\n",
    "    documents=pdf_documents,\n",
    "    collection_name=\"pdf-chroma\",\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Process Excel data\n",
    "excel_text = extract_data_from_excel(r\"C:\\Users\\Pritish\\Downloads\\Yearly_Returns.xlsx\")\n",
    "excel_chunks = split_text_into_chunks(excel_text)\n",
    "excel_documents = [Document(page_content=chunk) for chunk in excel_chunks]\n",
    "\n",
    "# Create vector store for Excel data\n",
    "excel_vectorstore = Chroma.from_documents(\n",
    "    documents=excel_documents,\n",
    "    collection_name=\"excel-chroma\",\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Set up retrievers\n",
    "pdf_retriever = pdf_vectorstore.as_retriever()\n",
    "excel_retriever = excel_vectorstore.as_retriever()\n",
    "\n",
    "# Configure the Groq LLM\n",
    "groq_api_key = \"xxx\"  # Ensure this key is correct\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Gemma2-9b-It\", temperature=0)\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"I will provide you pieces of [Context] to answer the [Question]. \n",
    "If you don't know the answer based on [Context], just say that you don't know. Don't make up an answer.\n",
    "[Context]: {context}\n",
    "[Question]: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Function to route query to the appropriate retriever\n",
    "def route_query(query, source_type):\n",
    "    if source_type == 'pdf':\n",
    "        retriever = pdf_retriever\n",
    "    elif source_type == 'excel':\n",
    "        retriever = excel_retriever\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported source type\")\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant documents found.\"\n",
    "    \n",
    "    context_from_docs = retrieved_docs[0].page_content  # Get content from the first relevant document\n",
    "\n",
    "    # Create the final prompt using the template\n",
    "    final_prompt = prompt.format(context=context_from_docs, question=query)\n",
    "\n",
    "    # Invoke the LLM directly with the formatted prompt\n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the document about?\"\n",
    "source_type = 'excel'  # Can be 'pdf' or 'excel'\n",
    "response = route_query(query, source_type)\n",
    "\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
